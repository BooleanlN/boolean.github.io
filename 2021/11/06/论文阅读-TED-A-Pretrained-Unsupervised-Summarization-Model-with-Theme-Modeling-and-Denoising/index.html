<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><title>论文阅读-TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising │ Boolean</title><link rel="stylesheet" href="/myblog/css/oasis.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/myblog/atom.xml" title="Boolean" type="application/atom+xml">
</head><body><div id="content"><h1 id="title">论文阅读-TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising</h1><div id="menu-outer"><nav id="menu-inner"><a id="menu-back" href="javascript:history.back()">Back</a><time datetime="2021-11-06T09:01:59.000Z">2021-11-06</time></nav></div><div id="content-outer"><div id="content-inner"><article id="post"><h2 id="TED-A-Pretrained-Unsupervised-Summarization-Model-with-Theme-Modeling-and-Denoising"><a href="#TED-A-Pretrained-Unsupervised-Summarization-Model-with-Theme-Modeling-and-Denoising" class="headerlink" title="TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising"></a>TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising</h2><p><em>一种基于主题建模和去噪的无监督预训练摘要生成模型</em></p>
<p><strong>{2020.EMNLP. Ziyi Yang. Chenguang Zhu}</strong></p>
<pre class="line-numbers language-none"><code class="language-none">@misc&#123;yang2020ted,
      title&#x3D;&#123;TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising&#125;, 
      author&#x3D;&#123;Ziyi Yang and Chenguang Zhu and Robert Gmyr and Michael Zeng and Xuedong Huang and Eric Darve&#125;,
      year&#x3D;&#123;2020&#125;,
      eprint&#x3D;&#123;2001.00725&#125;,
      archivePrefix&#x3D;&#123;arXiv&#125;,
      primaryClass&#x3D;&#123;cs.CL&#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><h3 id="Research-Objective-s"><a href="#Research-Objective-s" class="headerlink" title="Research Objective(s)"></a><strong>Research Objective(s)</strong></h3><ol>
<li>以往的模型基于RNN实现，而新提出的transformer性能更好</li>
<li>以往的摘要模型忽略了在大规模未标注语料的预训练</li>
</ol>
<p>基于上述问题，作者提出基于Transformer的无监督生成模型。模型基于以下思路：</p>
<ul>
<li>首先在未标注的大规模语料上进行训练</li>
<li>对TED模型基于主题模型与去噪自编码器提高生成摘要的质量</li>
</ul>
<h3 id="Background-Problem-Statement"><a href="#Background-Problem-Statement" class="headerlink" title="Background / Problem Statement"></a><strong>Background / Problem Statement</strong></h3><p>作者提出一种基于Transformer的无监督生成模型，贡献体现在：</p>
<ol>
<li><p>TED预训练：</p>
<p><strong>新闻文体采用倒金字塔结构，可用新闻的开头若干句，作为摘要内容</strong>。基于此假设，作者对模型在大规模未标注的语料上进行无监督预训练</p>
</li>
<li><p>TED finetune：</p>
<p>在具体的数据集上进行finetune，作者基于<strong>主题模型损失</strong>以及<strong>去噪自编码器</strong>进行finetune。</p>
<p>其中，主题模型loss目标是生成的文本与原始文本<strong>语义上更相近</strong>，而去噪自编码器目标是帮助模型从原始文本中，<strong>抽取主要信息</strong></p>
</li>
<li><p>为了解决生成模型常见的OOV问题，作者采用了<strong>SentencePiece tokenization</strong>^[1]^</p>
</li>
</ol>
<h3 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a><strong>Method(s)</strong></h3><ol>
<li>基于Transformer的Encoder-Decoder结构</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gx4d4pyey1j31fo0hodib.jpg" alt="image-20211206200009560" style="zoom:50%;" /></p>
<ol>
<li>无监督预训练</li>
<li>主题模型</li>
<li>去噪自编码器</li>
</ol>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><strong>Evaluation</strong></h3><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h3><h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a><strong>Notes</strong></h3><h3 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h3><p>[1] Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tok- enizer and detokenizer for neural text processing. <em>arXiv preprint arXiv:1808.06226</em>.</p>
<div class="post-tags"><a class="post-tag-link" href="/myblog/tags/%E8%AE%BA%E6%96%87/" rel="tag">#论文</a></div></article><div id="paginator"></div></div></div><div id="bottom-outer"><div id="bottom-inner"><hr><div><div><a>2021@ </a><a href="/atom.xml"><img src="/assets/rss.png"></a></div><div id="hexo"><a>Powered by&nbsp</a><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><a>&nbsp&&nbsp</a><a target="_blank" rel="noopener" href="https://github.com/qiantao94/hexo-theme-oasis">Oasis</a></div></div></div></div></div></body><script src="/myblog/js/oasis.js"></script></html>