<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>论文阅读-Heterogeneous Graph Neural Networks for Extractive Document Summarization</title>
  
  <link rel="canonical" href="http://example.com/2021/11/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Heterogeneous-Graph-Neural-Networks-for-Extractive-Document-Summarization/">
  
  <meta name="description" content="Heterogeneous Graph Neural Networks for Extractive Document SummarizationDanqing Wang∗, Pengfei Liu∗ ACL. 2020 @misc&amp;#123;wang2020heterogeneous,      ">
  
  
  <meta name="author" content="John Doe">
  
  <meta property="og:image" content="http://example.comundefined">
  
  <meta property="og:site_name" content="Hexo" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="论文阅读-Heterogeneous Graph Neural Networks for Extractive Document Summarization" />
  
  <meta property="og:description" content="Heterogeneous Graph Neural Networks for Extractive Document SummarizationDanqing Wang∗, Pengfei Liu∗ ACL. 2020 @misc&amp;#123;wang2020heterogeneous,      ">
  
  <meta property="og:url" content="http://example.com/2021/11/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Heterogeneous-Graph-Neural-Networks-for-Extractive-Document-Summarization/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="论文阅读-Heterogeneous Graph Neural Networks for Extractive Document Summarization">
  
  <meta name="twitter:description" content="Heterogeneous Graph Neural Networks for Extractive Document SummarizationDanqing Wang∗, Pengfei Liu∗ ACL. 2020 @misc&amp;#123;wang2020heterogeneous,      ">
  
  
  <meta name="twitter:image" content="http://example.comundefined">
  
  <meta name="twitter:url" content="http://example.com/2021/11/15/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Heterogeneous-Graph-Neural-Networks-for-Extractive-Document-Summarization/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="../fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="../fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hi Folks.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
          
          <a href="/About" class="ml">About</a>
          
        
        
          
            <a href="mailto:1363371357@qq.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>论文阅读-Heterogeneous Graph Neural Networks for Extractive Document Summarization</h2>

  <h2 id="Heterogeneous-Graph-Neural-Networks-for-Extractive-Document-Summarization"><a href="#Heterogeneous-Graph-Neural-Networks-for-Extractive-Document-Summarization" class="headerlink" title="Heterogeneous Graph Neural Networks for Extractive Document Summarization"></a>Heterogeneous Graph Neural Networks for Extractive Document Summarization</h2><p><em>Danqing Wang∗, Pengfei Liu∗ ACL. 2020</em></p>
<pre class="line-numbers language-none"><code class="language-none">@misc&#123;wang2020heterogeneous,
      title&#x3D;&#123;Heterogeneous Graph Neural Networks for Extractive Document Summarization&#125;, 
      author&#x3D;&#123;Danqing Wang and Pengfei Liu and Yining Zheng and Xipeng Qiu and Xuanjing Huang&#125;,
      year&#x3D;&#123;2020&#125;,
      eprint&#x3D;&#123;2004.12393&#125;,
      archivePrefix&#x3D;&#123;arXiv&#125;,
      primaryClass&#x3D;&#123;cs.CL&#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><a target="_blank" rel="noopener" href="https://github.com/dqwang122/HeterSumGraph">代码地址</a></p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><p>作者首次提出一种用于抽取式文本摘要刻画句子间关系的异构图，图中包含了<strong>词节点</strong>（基本语义单元）、<strong>句子结点</strong>（以及多文档摘要时用到的文档节点），词节点通过<strong>词嵌入</strong>做初始化，句子节点则通过<strong>CNN+BiLSTM</strong>抽取特征向量，以词节点的<strong>TF-IDF值</strong>作为词节点与句子节点之间的边。</p>
<p>之后，首先对句子结点值，通过GAT + FNN 进行聚合更新，再用更新后的句子结点对词结点进行更新，最后，对句子进行分类（留或不留），并用Trigram Blocking防止冗余。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwrgywuvjvj30lg0am3yy.jpg" alt="image-20211125162038154" style="zoom:50%;" />

<p>结果证明了作者提出的异构图在抽取句子关系时的效果，同时，在CNN/DailyMail等数据集中，表现要优于那些没有使用预训练模型，如BERT的模型。（因为计算资源限制，作者未引入BERT）</p>
<h3 id="Research-Objective-s"><a href="#Research-Objective-s" class="headerlink" title="Research Objective(s)"></a><strong>Research Objective(s)</strong></h3><p>在抽取式文本摘要领域，为了从一个文档中有效地抽取出值得作为总结的句子，需要对句子之间的关系进行刻画。作者第一个<strong>在基于图的神经网络方法中引入了不同的节点类型来解决文本摘要任务</strong>，并对引入其他类型节点的好处进行了分析。</p>
<p>we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. </p>
<h3 id="Background-Problem-Statement"><a href="#Background-Problem-Statement" class="headerlink" title="Background / Problem Statement"></a><strong>Background / Problem Statement</strong></h3><p>文本摘要分为生成式与抽取式，在抽取式文档摘要领域，目标是能够从原始文档中抽取出有重大意义的句子。为了抽取句子，其中一个重要的步骤就是刻画句子间关系，常用的方法有LexRank^[1]^、TextRank^[2]^，基于Approximate Discourse Graph(ADG)^[3]^，基于Rhetorical Structure Theory（RST）^[4]^。但这些方法常常依赖于外部工具，并需要考虑错误传播问题。</p>
<p>Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem.</p>
<p>尽管这些模型取得了不错的成绩，但如何为文本摘要构建一个有效的图结构仍然是一个亟待解决的问题。</p>
<h3 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a><strong>Method(s)</strong></h3><p>作者引入了更多语义单元作为额外的节点来丰富句子之间的关系。作者选择单词作为语义单元，句子与句子之间没有直接相连的边，这种图结构具有以下优点：</p>
<ol>
<li>不同的句子之间可以通过重叠词信息相互作用</li>
<li>词节点从句子中聚合信息，并进行更新，而其他模型word embedding通常是不变的 </li>
<li>通过多个消息传递过程，可以充分利用不同粒度的信息。</li>
<li>作者提出的异构图是可扩展的，例如，可以为多文档摘要任务引入文档结点。</li>
</ol>
<p>具体方法：</p>
<p>给定文档$D = {s_1,s_2,…,s_n}$，我们的目标是预测一系列标注结果$Y = {y_1,y_2,…,y_n}，y_i \in {0,1}$，其中，$y_i = 1$表示该句子$s_i$应该包含在结果中。</p>
<p>在作者构建的异构图中，基本语义节点作为中继节点，其他单元作为super 结点，super结点与它包含的基本结点相连接，连接的权重为它们关系的重要程度。</p>
<p>basic semantic nodes (e.g. words, concepts, etc.) as relay nodes and other units of discourse (e.g. phrases, sentences, documents, etc.) as supernodes. Each supernode connects with basic nodes contained in it and takes the importance of the relation as their edge feature. Thus, high-level discourse nodes can establish relationships between each other via basic nodes.</p>
<img src="/Users/jiayi/Library/Application%20Support/typora-user-images/image-20211123231937172.png" alt="image-20211123231937172" style="zoom:50%;" />

<p>如上图所示，作者提出的模型包括三部分：图初始化器、异构图层和句子选择器。</p>
<p><strong>图初始化器：</strong></p>
<p>Word Encoder：$X_w \in R^{m \times d_w }$，词嵌入，word embedding，$d_w$是向量维度</p>
<p>Sentence Encoder：$X_s \in R^{n \times d_s}$，$d_s$为向量维度，其中首先通过CNN获取局部n-gram特征$l_j$，再通过BiLSTM获取句子级别的特征$g_j$，将两者相练，得到最后的向量$X_{sj} = [l_j;g_j]$</p>
<p>Edge Initializer：使用TF-IDF值，作为边的初始权重。</p>
<p><strong>异构图层：</strong></p>
<p>使用GAT^[6]^（graph attention networks）对语义单元结点进行更新。</p>
<p>$z_{ij} = LeakyReLU(W_a[W_qh_i;W_kh_j;e_{ij}])$</p>
<p>$\alpha_{ij} = \frac{exp(z_{ij})}{\sum_{l \in N_i}exp(z_{il})}$</p>
<p>$u_i = \sigma(\sum_{j \in N_i}{\alpha_{ij}W_vh_j})$</p>
<p>在这里，作者采用<strong>多头self-attention</strong>：</p>
<p>$u_i = ||<em>{k=1}^K\sigma(\sum</em>{j \in N_i}{\alpha_{ij}W_vh_j})$</p>
<p>除此之外，增加了<strong>残差连接</strong>解决梯度消失的问题：</p>
<p>$h’_i = u_i + h_i$</p>
<p>在每层graph attention layer之后，引入位置前馈层（position-wise FFN），该层由两个线性变换组成，类似Transformer那样。</p>
<p>之后，如下图所示，句子节点由相连接的词节点通过GAT与FFN进行聚合更新。更新完句子节点后，词节点再由更新后的句子节点进行聚合更新。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwq6ltdd3sj30pc0mewhl.jpg" alt="image-20211124133631315" style="zoom:50%;" />

<p>最终，出现次数多的单词节点会有更高的degree，表明它可能是该文章的keyword，包含更多keyword的句子理所当然地具备更高的优先级作为输出句子。</p>
<p><strong>GAT：</strong></p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwq5eyy671j31240jywg8.jpg" alt="image-20211124125519304" style="zoom:50%;" />

<p>单层Graph Attentional Layer输入节点特征向量集：$h = {h_1,h_2,…,h_n},h_i \in R^F$，其中，F为向量维度</p>
<p>每一层输出为一个新的节点特征向量集合：$h’ = {h’_1,h’_2,…,h’_n},h’_i \in R^{F’}$，其中，F’为新的向量维度</p>
<p>具体来讲，Graph attentional layer会对输入特征向量进行<strong>self-attention</strong>：$e_{ij} = a(Wh_i, Wh_j)$，其中a是从$R^{F’} \times R^{F’} -&gt; R$的映射，W是被所有$h_i$<strong>共享</strong>的$R^{F’ \times F}$的权值矩阵。这样的操作会将注意力分配到图中的所有节点当中，这样做会丢失结构信息。GAT采用一种masked attention的方式，仅将注意力分配到节点i的邻节点集上。</p>
<p>$\alpha_{ij} = softmax(e_ij) = \frac{exp(e_{ij})}{\sum_{k \in N_i}exp(e_{ik})}$，其中，$N_i$是结点i的邻近结点集合。</p>
<p>在<em>Graph Attention Networks</em>中，作者采取了一层前馈网络作为a的实现：</p>
<p>$\alpha_{ij} = \frac{exp(LeakyReLU(a^T[Wh_i||Wh_j]))}{\sum_{k \in N_i}exp(LeakyReLU(a^T[Wh_i||Wh_k]))} $</p>
<p>之后，就可以求得$h’<em>i$：$h’<em>i = \sigma(\sum</em>{j \in N_i}{\alpha</em>{ij}Wh_j})$</p>
<p>此外，为了提高模型的拟合能力，作者引入了多头self-attention，具体实现即同时采用多个$W_k$，之后将结果进行合并或求均值。</p>
<p>$h_i = {||}<em>{k=1}^K \sigma(\sum</em>{j \in N_i}{\alpha_{ij}W^kh_j})$，这是连接方式，还可以通过均分等方式：</p>
<p>$h_i = \sigma(\frac{1}{K}\sum_{k=1}^K\sum_{j \in N_i}{\alpha_{ij}W^kh_j})$</p>
<p><strong>句子选择器：</strong></p>
<p>对句子节点进行分类，交叉熵损失函数作为训练目标。</p>
<p>使用<strong>Trigram Blocking</strong>^[5]^，一个MMR方法进行句子去重，对句子进行排序，去掉与前序句子有三叉重合的句子。</p>
<p><strong>多文档摘要：</strong></p>
<p>如下图所示，作者通过在异构图中，引入文档节点，通过word node，以<strong>TF-IDF值</strong>作为edge，可以建立起文档与文档之间的层次关系。</p>
<p>文档结点与句子结点的更新步骤是一样的，与句子结点不同的是，文档结点使用它<strong>包含的句子结点的均值池化值</strong>作为初始值。</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwrfezzk1hj30ou0qu42a.jpg" alt="image-20211125152651920" style="zoom:50%;" />

<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><strong>Evaluation</strong></h3><p><strong>数据集</strong>：<strong>CNN/DailyMail</strong>、<strong>NYT50</strong>、<strong>Multi-News</strong>（多文档摘要）</p>
<p><strong>word embedding</strong>：Glove.300d</p>
<p><strong>optimizer:</strong> Adam</p>
<p><strong>评价指标：</strong>Rouge-1、Rouge-2、Rouge-L</p>
<p><strong>Baselines</strong>：</p>
<p>作者用三种抽取句子间关系的方法进行实验，以此证明<strong>HETERSUMGARPH</strong>在句子关系抽取中的效果更加powerful。</p>
<ul>
<li><p>**Ext-BiLSTM **：通过CNN+BiLSTM得到的句子向量直接进行classification</p>
</li>
<li><p><strong>Ext-Transformer</strong>：通过Transformer结构对完全连接的句子学习其pairwise交互结构得到encoder</p>
</li>
<li><p><strong>HETERSUMGARPH</strong></p>
</li>
</ul>
<p>如下图所示是作者提出的模型在CNN/DailyMail的实验对比结果，可以看出相比于其它的句子关系抽取方法，<strong>作者提出的方法效果更好</strong>。</p>
<p>并且，作者提出的模型性能要<strong>优于之前所有未以BERT为基础的摘要模型</strong>，并证明了<strong>Trigram blocking</strong>对ROUGE评分具有很好的提升效果，</p>
<img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwrg6qk4e1j30na0o6n1k.jpg" alt="image-20211125155332812" style="zoom:50%;" />



<h3 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h3><p>[1] Gu ̈nes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. <em>Journal of artificial intelligence re- search</em>, 22:457–479.</p>
<p>[2] Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring- ing order into text. In <em>Proceedings of the 2004 con- ference on empirical methods in natural language processing</em>, pages 404–411.</p>
<p>[3] Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir Radev. 2017. Graph-based neural multi-document summarization. <em>arXiv preprint arXiv:1706.06681</em>.</p>
<p>[4] Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu. 2019. Discourse-aware neural extractive model for text summarization. <em>arXiv preprint arXiv:1910.14142</em>.</p>
<p>[5] Yang Liu and Mirella Lapata. 2019b. Text summariza- tion with pretrained encoders. In <em>Proceedings of the 2019 Conference on Empirical Methods in Nat- ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3721–3731, Hong Kong, China. Association for Computational Linguistics.</p>
<p>[6] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. <em>arXiv preprint arXiv:1710.10903</em>.</p>

  <p><a class="classtest-link" href="/tags/%E8%AE%BA%E6%96%87/" rel="tag">论文</a> — Nov 15, 2021</p>
  


          <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
        at <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/BooleanlN" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      

      

      

    </div>
  
</div>

        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>
