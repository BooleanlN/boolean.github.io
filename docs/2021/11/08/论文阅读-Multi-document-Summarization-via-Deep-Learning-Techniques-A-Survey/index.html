<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><title>è®ºæ–‡é˜…è¯»-Multi-document Summarization via Deep Learning Techniques: A Survey â”‚ Boolean</title><link rel="stylesheet" href="/myblog/css/oasis.css"><meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/myblog/atom.xml" title="Boolean" type="application/atom+xml">
</head><body><div id="content"><h1 id="title">è®ºæ–‡é˜…è¯»-Multi-document Summarization via Deep Learning Techniques: A Survey</h1><div id="menu-outer"><nav id="menu-inner"><a id="menu-back" href="javascript:history.back()">Back</a><time datetime="2021-11-08T09:07:02.000Z">2021-11-08</time></nav></div><div id="content-outer"><div id="content-inner"><article id="post"><h2 id="Multi-document-Summarization-via-Deep-Learning-Techniques-A-Survey"><a href="#Multi-document-Summarization-via-Deep-Learning-Techniques-A-Survey" class="headerlink" title="Multi-document Summarization via Deep Learning Techniques: A Survey"></a>Multi-document Summarization via Deep Learning Techniques: A Survey</h2><p><em>åŸºäºæ·±åº¦å­¦ä¹ çš„å¤šæ–‡æ¡£æ‘˜è¦æ¨¡å‹ç»¼è¿°</em></p>
<p><strong>Computation and Language. 2020. <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Ma,+C">Congbo Ma</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/search/cs?searchtype=author&query=Zhang,+W+E">Wei Emma Zhang</a></strong></p>
<pre class="line-numbers language-none"><code class="language-none">@misc&#123;ma2020multidocument,
      title&#x3D;&#123;Multi-document Summarization via Deep Learning Techniques: A Survey&#125;, 
      author&#x3D;&#123;Congbo Ma and Wei Emma Zhang and Mingyu Guo and Hu Wang and Quan Z. Sheng&#125;,
      year&#x3D;&#123;2020&#125;,
      eprint&#x3D;&#123;2011.04843&#125;,
      archivePrefix&#x3D;&#123;arXiv&#125;,
      primaryClass&#x3D;&#123;cs.CL&#125;
&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><h3 id="Research-Objective-s"><a href="#Research-Objective-s" class="headerlink" title="Research Objective(s)"></a><strong>Research Objective(s)</strong></h3><p>ä½œè€…çš„ç ”ç©¶ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<h3 id="Background-Problem-Statement"><a href="#Background-Problem-Statement" class="headerlink" title="Background / Problem Statement"></a><strong>Background / Problem Statement</strong></h3><p>ç ”ç©¶çš„èƒŒæ™¯ä»¥åŠé—®é¢˜é™ˆè¿°ï¼šä½œè€…éœ€è¦è§£å†³çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ</p>
<h3 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a><strong>Method(s)</strong></h3><p>å¤šæ–‡æ¡£æ‘˜è¦çš„ç›®æ ‡æ˜¯ä»ä¸€æ‰¹æ–‡æ¡£é›†åˆDä¸­ç”Ÿæˆä¸€ä¸ªç®€æ´ä¸”ä¿¡æ¯ä¸°å¯Œçš„æ‘˜è¦Sumã€‚æ–‡æ¡£é›†åˆDæ˜¯ä¸»é¢˜ç›¸å…³çš„ä¸€ç³»åˆ—æ–‡æœ¬ã€‚</p>
<p>The aim of multi- document summarization is to generate a concise and informative summary ğ‘†ğ‘¢ğ‘š from a collection ofdocumentsğ·.ğ·denotesaclusteroftopic-relateddocuments{ğ‘‘ğ‘– |ğ‘–âˆˆ[1,ğ‘]},whereğ‘isthe number of documents. Each document ğ‘‘ğ‘– consists of ğ‘€ sentences ô°ˆğ‘ ğ‘–,ğ‘— | ğ‘— âˆˆ [1, ğ‘€]ô°‰. ğ‘ ğ‘–,ğ‘— refers to the ğ‘—-th sentence in the ğ‘–-th document. </p>
<p>ä½œè€…å¯¹å¤šæ–‡æ¡£æ‘˜è¦çš„æµç¨‹è¿›è¡Œäº†æ€»ç»“ä¸æ¢³ç†ï¼Œé€šå¸¸åŒ…æ‹¬1âƒ£ï¸é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„æ–‡æ¡£è¿æ¥æ–¹æ³•2âƒ£ï¸å¯¹æ–‡æ¡£è¿›è¡Œé¢„å¤„ç†3âƒ£ï¸é€šè¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹è·å–è¯­ä¹‰ä¸°å¯Œçš„è¡¨è¾¾4âƒ£ï¸èåˆå„ç§ç±»å‹çš„è¡¨è¾¾ï¼Œ5âƒ£ï¸å¥å­é€‰æ‹©æˆ–æ‘˜è¦ç”Ÿæˆ</p>
<p>The first step is to select an appropriate concatenation approach for input documents. The second step is pre-processing these documents, such as segmenting sentences, tokenizing non-alphabetic characters and removing punctuations [118]. Then, an appropriate deep learning based model is chosen to generate semantic rich representation for downstream tasks. The next step is to fuse these various types of representation for later sentence selection or summary generation. Finally, through these five steps, multiple documents are transformed into concise and informative summaries.</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gw7z2428ezj312u0aojsn.jpg" alt="image-20211108193513722"></p>
<h4 id="input-document-types-è¾“å…¥æ–‡æ¡£ç±»å‹"><a href="#input-document-types-è¾“å…¥æ–‡æ¡£ç±»å‹" class="headerlink" title="input document types è¾“å…¥æ–‡æ¡£ç±»å‹"></a>input document types è¾“å…¥æ–‡æ¡£ç±»å‹</h4><ul>
<li><p>å¤šç¯‡çŸ­æ–‡æ¡£ Many short documents.</p>
<p>æ¯ç¯‡æ–‡æ¡£é•¿åº¦ç›¸å¯¹è¾ƒçŸ­ï¼Œä½†æ•°é‡å¾ˆå¤§ï¼Œå¦‚äº§å“è¯„è®ºç”Ÿæˆã€‚</p>
</li>
<li><p>å°‘é‡é•¿æ–‡æ¡£ Few long documents.</p>
<p>æ¯ç¯‡æ–‡æ¡£é•¿åº¦ç›¸å¯¹è¾ƒé•¿ï¼Œä½†æ•°é‡è¾ƒå°‘ï¼Œå¦‚æ–°é—»æ‘˜è¦ç”Ÿæˆã€‚</p>
</li>
<li><p>æ··åˆæ–‡æ¡£ç±»å‹ Hybrid documents. </p>
<p>ä¸€ç¯‡æˆ–å¤šç¯‡é•¿æ–‡æ¡£åŒæ—¶ä¼´éšç€è‹¥å¹²ç¯‡çŸ­æ–‡æ¡£ï¼Œå¦‚æ–°é—»ç¨¿+æ–°é—»çŸ­è¯„ï¼Œè®ºæ–‡+ç®€çŸ­çš„å¼•ç”¨ã€‚</p>
</li>
</ul>
<h4 id="Concatenation-Methods-è¿æ¥æ–¹æ³•"><a href="#Concatenation-Methods-è¿æ¥æ–¹æ³•" class="headerlink" title="Concatenation Methods è¿æ¥æ–¹æ³•"></a>Concatenation Methods è¿æ¥æ–¹æ³•</h4><ol>
<li><p>Flat Concatenation å¹³æ»‘è¿æ¥</p>
<p>å°†æ‰€æœ‰è¾“å…¥æ–‡æ¡£å›Šæ‹¬åœ¨ä¸€ä¸ªè¾“å…¥å½“ä¸­ï¼Œå¹¶å½“ä½œä¸€ä¸ªæ•´ä½“çš„sequenceè¿›è¡Œå¤„ç†ï¼Œç®€å•ä½†powerfulã€‚</p>
<p> All input documents are spanned and are processed as a flat sequence.</p>
</li>
<li><p>Hierarchical Concatenation åˆ†å±‚è¿æ¥</p>
<p>å€ŸåŠ©åˆ†å±‚è¿æ¥ï¼Œæœ‰åŠ©äºæ¨¡å‹è·å¾—è¯­ä¹‰ä¸°å¯Œçš„è¡¨ç¤ºï¼Œä»è€Œæé«˜æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚å½“å‰çš„åˆ†å±‚è¿æ¥æ–¹æ³•ï¼ŒåŒ…å«ä¸¤ç§æ–¹æ¡ˆï¼š</p>
<ul>
<li><p>æ–‡æ¡£çº§åˆ«ï¼šåˆ†åˆ«å¾—åˆ°æ–‡æ¡£çš„æ‘˜è¦è¡¨ç¤ºï¼Œè¿™äº›è¡¨ç¤ºåœ¨åç»­è¿‡ç¨‹ä¸­è¿›è¡Œèåˆã€‚</p>
<p>For the document-level concatenation methods, a condense model [3] is proposed to learn document-level representation separately in a cluster and these representation are fused in the subsequent processes.</p>
</li>
<li><p>å•è¯/å¥å­çº§åˆ«ï¼šåŒ…å«èšç±»ä¸å…³ç³»å›¾æ–¹æ³•</p>
<p><strong>èšç±»æ–¹æ³•ï¼š</strong>é¦–å…ˆå¯¹è¾“å…¥å¯¹æ–‡æ¡£è¿›è¡Œå¥å­çº§èšç±»ï¼Œç„¶åä»ä¸åŒçš„èšç±»ä¸­é€‰æ‹©å¥å­ï¼Œä¿è¯æœ€å¤šä»ä¸€ä¸ªèšç±»ä¸­é€‰æ‹©å‡ºä¸€ä¸ªå¥å­ï¼Œä»è€Œå‡å°‘å†—ä½™ä¿¡æ¯ï¼Œå¹¶æé«˜ä¿¡æ¯çš„è¦†ç›–ç‡ã€‚</p>
<p>First allowing the model group related sentences. Then, the model selects sentences from diverse clusters, and at most one sentence will be selected from a cluster. By doing so, it will decrease redundancy and increase the information coverage for the generated summaries.</p>
<p><strong>å…³ç³»å›¾æ–¹æ³•ï¼š</strong>æ„é€ å¥å­å…³ç³»å›¾æ¥è¡¨ç¤ºæ–‡æ¡£ä¹‹é—´çš„å±‚æ¬¡å…³ç³»ï¼Œå¸¸ç”¨çš„æ„é€ æ–¹æ³•æœ‰ä½™å¼¦ç›¸ä¼¼å›¾ã€è¿‘ä¼¼è¯­ç¯‡å›¾ä»¥åŠä¸ªæ€§åŒ–è¯­ç¯‡å›¾ã€‚è¿˜æœ‰ä¸€ç§å¼‚æ„å›¾æ¨¡å‹åˆ©ç”¨å•è¯æ„é€ å¥å­ä¸å¥å­ã€å¥å­ä¸æ–‡æ¡£ä¹‹é—´çš„å…³ç³»å›¾ã€‚</p>
<p>Cosine similarity graph, approximate discourse graph, and personalized discourse graph are the most commonly used methods recently for building sentence graph structures. The heterogeneous graph model leverages words as intermediate nodes to construct a document-document, sentence-sentence and sentence-document hierarchical structure.</p>
</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gw7wtp5l44j310w0u00x5.jpg" alt="image-20211108181754273"></p>
</li>
</ol>
<h4 id="Summarization-Construction-Types-æ‘˜è¦æ„é€ æ–¹æ³•"><a href="#Summarization-Construction-Types-æ‘˜è¦æ„é€ æ–¹æ³•" class="headerlink" title="Summarization Construction Types æ‘˜è¦æ„é€ æ–¹æ³•"></a>Summarization Construction Types æ‘˜è¦æ„é€ æ–¹æ³•</h4><ol>
<li>abstractive summarization ç”Ÿæˆå¼æ‘˜è¦</li>
<li>extractive summarization æŠ½å–å¼æ‘˜è¦</li>
<li>hybrid summarization æ··åˆæ–¹æ³•æ‘˜è¦</li>
</ol>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gw7yphbejuj313w0fe0wb.jpg" alt="image-20211108192305082"></p>
<p>ä½œè€…æå‡ºäº†ä¸€ç§æ¦‚æ‹¬æ¨¡å‹ç»“æ„çš„æ–°æ–¹æ³•ï¼Œå°†æ¨¡å‹ç»“æ„åˆ†ä¸ºäº†ä»¥ä¸‹ç±»å‹ï¼š</p>
<p>å…¶ä¸­ï¼Œç»¿è‰²è™šçº¿boxå¯ä»¥ç”±å…¶å®ƒç½‘ç»œæ¨¡å‹çµæ´»ä»£æ›¿ï¼Œè“è‰²å®çº¿boxåˆ™è¡¨æ˜é€šè¿‡ç¥ç»ç½‘ç»œæˆ–å¯å‘å¼è®¾è®¡æ–¹æ³•è¿›è¡Œçš„åµŒå…¥æ“ä½œï¼Œå¯ä»¥æ˜¯å¥å­/æ–‡æ¡£ç­‰ç±»å‹çš„è¡¨è¾¾ã€‚</p>
<p> In this figure, deep neural models are boxed in green dotted line, which can be flexibly substituted by other backbone networks. The blue solid line boxes indicate the neural embeddings processed by neural networks or heuristic-designed approaches. It can be sentence/document representation or other types of representation. </p>
<ul>
<li><p>Naive Networks</p>
<p>æœ´ç´ ç½‘ç»œï¼ŒDNN modelä½œç”¨æ˜¯ä¸€ä¸ªç‰¹å¾æå–å™¨ï¼Œå¾—åˆ°çš„representationäº¤ç”±ä¸‹æ¸¸åšå¥å­é€‰æ‹©æˆ–æ‘˜è¦ç”Ÿæˆã€‚</p>
</li>
<li><p>Ensemble Networks.</p>
<p>é›†æˆç½‘ç»œï¼Œè¾“å…¥docuemntsè‡³å¤šä¸ªæ¨¡å‹ï¼Œä¹‹åå„æ¨¡å‹å¾—åˆ°çš„è¡¨è¾¾ä¼šè¿›è¡Œèåˆæ¥æé«˜æ¨¡å‹æ•´ä½“çš„è¡¨è¾¾èƒ½åŠ›ã€‚ä¸»æµèåˆæ–¹æ³•æœ‰æŠ•ç¥¨æˆ–å‡å€¼ã€‚</p>
<p>Ensemble networks feed input documents to multiple paths with different network structures or operations. Later on, these representations are fused to enhance model expression capability. The majority vote or average can be used to determine the final solution.</p>
</li>
<li><p>Auxiliary Task Networks</p>
</li>
<li><p>Reconstruction Networks</p>
</li>
<li><p>Fusion Networks. Fusion networks </p>
</li>
<li><p>Graph Neural Networks</p>
</li>
<li><p>Hierarchical Networks</p>
<p>æ–‡æ¡£é›†è¿æ¥åè¾“å…¥ç¬¬ä¸€å±‚DNNåï¼Œè·å¾—å…¶è¡¨å±‚ç‰¹å¾ï¼Œä¹‹åå°†è¯¥ç¬¬ä¸€å±‚è¾“å‡ºä½œä¸ºç¬¬äºŒå±‚DNNçš„è¾“å…¥ï¼Œç”Ÿæˆæ·±å±‚æ¬¡è¡¨å¾ï¼Œåˆ†å±‚æ¨¡å‹å¯ä»¥æ›´æœ‰æ•ˆåœ°æå–æŠ½è±¡å±‚æ¬¡å’Œè¯­ä¹‰ç‰¹å¾ã€‚</p>
<p>The Hierarchical networks empower the model with the ability to capture abstract-level and semantic-level features more efficiently.</p>
</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gw7z7p99rdj310p0u0aes.jpg" alt="image-20211108194035375"></p>
<h4 id="RNN-based-Models"><a href="#RNN-based-Models" class="headerlink" title="RNN based Models"></a>RNN based Models</h4><h4 id="CNN-based-Models"><a href="#CNN-based-Models" class="headerlink" title="CNN based Models"></a>CNN based Models</h4><h4 id="GNN-based-Models"><a href="#GNN-based-Models" class="headerlink" title="GNN based Models"></a>GNN based Models</h4><p>è‡ªç„¶è¯­è¨€æ•°æ®é€šå¸¸ç”±å…³ç³»å¯†åˆ‡çš„è¯æ±‡å’ŒçŸ­è¯­ç»„æˆï¼Œç›¸æ¯”äºåºåˆ—çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå›¾ç»“æ„èƒ½å¤Ÿæ›´å¥½åœ°è¿›è¡Œè¡¨ç¤ºã€‚</p>
<p>Natural language data consist of vocabularies and phrases with strong relations and they can be better represented with graphs rather than in sequential orders</p>
<ol>
<li><p>åŸºäºGCNçš„æ–¹æ³•^[1]^ ï¼šæ„å»ºå¥å­å…³ç³»å›¾ï¼Œé€å…¥GCNè·å¾—å¥å­ç›¸å…³ç‰¹å¾ã€‚</p>
<p>This model first builds a sentence-based graph and then feeds the pre-processed data into a GCN [60] to capture the sentence-wise related features.</p>
<p><strong>æ„é€ å›¾çš„æ–¹æ³•ï¼š</strong>æ¯ä¸ªå¥å­è§†ä½œä¸€ä¸ªç»“ç‚¹ï¼Œå¥å­é—´çš„å…³ç³»ä¸ºè¾¹ï¼Œå…³ç³»æœ‰ä½™å¼¦ç›¸ä¼¼åº¦ã€è¿‘ä¼¼è¯­ç¯‡ã€ä¸ªæ€§åŒ–è¯­ç¯‡ç­‰ã€‚</p>
<p>Defined by the model, each sentence is regarded as a node and the relation between each pair of sentences is defined as an edge. Inside each document cluster, the sentence relation graph can be generated through cosine similarity graph [32], approximate discourse graph [23] and the proposed personalized discourse graph. </p>
<p>å¥å­å…³ç³»å›¾ä»¥åŠRNNæŠ½å–å‡ºembeddingï¼Œéƒ½é€å…¥å›¾å·ç§¯ç¥ç»ç½‘ç»œæ¥å¾—åˆ°å¥å­æœ€ç»ˆçš„è¡¨è¾¾ã€‚æœ€åå°†è¾“å‡ºé€å…¥<strong>æ–‡æ¡£GRU</strong>å®Œæˆé›†ç¾¤åµŒå…¥ï¼Œå®Œå…¨èšåˆå¥å­ä¹‹é—´çš„ç‰¹å¾ã€‚</p>
<p>Both of the sentence relation graph and sentence embeddings extracted by a sentence-level RNN are fed into graph convolution networks to produce the final sentence representation. With the help of a document-level GRU, the model generates cluster embeddings to fully aggregate features between sentences.</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gw95ox3f02j31440f4q4w.jpg" alt="image-20211109201016184"></p>
</li>
<li><p>åŸºäºå¼‚æ„å›¾çš„æ–¹æ³•^[3]^ ï¼šä¸Šé¢çš„æ–¹æ³•åªåˆ©ç”¨äº†å¥å­çº§çš„å…³ç³»ï¼Œæ²¡æœ‰å……åˆ†è€ƒè™‘å•è¯ã€å¥å­ä»¥åŠæ–‡ç« ä¹‹é—´çš„å…³ç³»ã€‚</p>
<p>The existing graph neural networks based models are mainly focused on the relationship between sentences and do not fully consider the relations among words, sentences and documents</p>
<p> HeterDoc-Sum Graph^[3]^æ˜¯ä¸€ç§åŸºäºå¼‚æ„å›¾æ³¨æ„åŠ›ç½‘ç»œï¼Œå®ƒåŒ…å«äº†å•è¯èŠ‚ç‚¹ã€å¥å­èŠ‚ç‚¹ä»¥åŠæ–‡ç« èŠ‚ç‚¹ï¼Œå¥å­èŠ‚ç‚¹å’Œæ–‡ç« èŠ‚ç‚¹é€šè¿‡å…±ç°å•è¯å…³ç³»ç›¸è¿æ¥</p>
<p>Sentence nodes and document nodes are connected according to the contained word nodes. </p>
<p>TF-IDFå€¼ä½œä¸ºå•è¯-å¥å­ä»¥åŠå•è¯-æ–‡ç« çš„æƒé‡</p>
<p>ä¸‰ç§å…³ç³»å›¾é€å…¥å›¾æ³¨æ„åŠ›ç½‘ç»œ^[2]^ä¸­è¿›è¡Œæƒé‡æ›´æ–°ï¼Œæ¯æ¬¡æ›´æ–°æ—¶ï¼Œå¯¹å•è¯å¥å­å’Œå•è¯æ–‡æ¡£è¿›è¡ŒåŒå‘æ›´æ–°ï¼Œä»¥æ›´å¥½åœ°èšåˆè·¨å±‚è¯­ä¹‰çŸ¥è¯†ã€‚</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwaau91zzlj30pk0qkjv6.jpg" alt="image-20211110195359995"></p>
</li>
</ol>
<h4 id="PGNet-based-Models"><a href="#PGNet-based-Models" class="headerlink" title="PGNet based Models"></a>PGNet based Models</h4><p>Pointer-generator ç½‘ç»œè¢«æå‡ºæ¥è§£å†³æ‘˜è¦é¢†åŸŸçš„äº‹å®é”™è¯¯ä»¥åŠé«˜å†—ä½™æ€§é—®é¢˜ã€‚</p>
<p>Pointer-generator networks is proposed to overcome the problems of factual errors and high redundancy in the summarization task. </p>
<p>è¿™ç±»ç½‘ç»œæ¶æ„å—åˆ°äº†pointer networkã€copynetã€forced-attention sentence compression ä»¥åŠ coverage mechanismçš„å¯å‘ã€‚å¯¹è¿™äº›æ¦‚å¿µè¿›è¡Œä»‹ç»ï¼š</p>
<p><strong>pointer network</strong></p>
<p><strong>copynet</strong></p>
<p><strong>converage mechanism</strong></p>
<p><strong>MMRï¼ˆMaximal Marginal Relevanceï¼‰</strong><br>$$<br>MMR = ArgMAX_{Di\ in \ æœªé€‰ä¸­é›†åˆ}[\lambda Sim_1(D_i,Q) - (1-\lambda)max_{Dj \ in \ å·²é€‰ä¸­é›†åˆ}Sim_2(Di,D_j))]<br>$$<br><em>å…¶ä¸­Qä¸ºç”¨æˆ·ï¼Œå‰åŠéƒ¨åˆ†åœ¨æ‘˜è¦é¢†åŸŸæ˜¯å¯¹å¥å­çš„æ‰“åˆ†ã€‚</em></p>
<p>è¯¥æ–¹æ³•ç›®çš„åœ¨äºå‡å°‘æ’åºç»“æœçš„å†—ä½™ï¼Œä¿è¯ç»“æœçš„å¤šæ ·æ€§ï¼Œå¸¸ç”¨äºæ¨èé¢†åŸŸã€‚</p>
<p>More accurately, the MMR scores are multiplied to the original attention weights. MMR method is designed to select a set of salience sentences from source documents by considering both importance and redundancy indexes</p>
<h4 id="Encoder-decoder-based-Models"><a href="#Encoder-decoder-based-Models" class="headerlink" title="Encoder-decoder based Models"></a>Encoder-decoder based Models</h4><p><strong>Encoderï¼š</strong>å¯¹åŸæ–‡æ¡£è¿›è¡Œç¼–ç è¡¨ç¤ºï¼Œç¼–ç åŒ…å«äº†å‹ç¼©è¯­ä¹‰å’Œå¥æ³•ä¿¡æ¯</p>
<p><strong>Decoderï¼š</strong>å¯¹encoderçš„ç¼–ç ç»“æœè¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆç›®æ ‡æ‘˜è¦</p>
<p>For multi-document summarization, the encoder embeds source documents into the hidden representations, i.e., word representation, sentence representation and document representation. Then, the representation containing compressed semantic and syntactic information is passed to the decoder to generate the target summaries. </p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwackd5wj8j312g0ckabf.jpg" alt="image-20211110205341614"></p>
<h4 id="Variational-Auto-Encoder-based-Models"><a href="#Variational-Auto-Encoder-based-Models" class="headerlink" title="Variational Auto-Encoder based Models"></a>Variational Auto-Encoder based Models</h4><p>å˜åˆ†è‡ªç¼–ç å™¨ï¼š</p>
<p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gwacpjz9ulj315s09wgn2.jpg" alt="image-20211110205840492"></p>
<h4 id="Transformer-based-Models"><a href="#Transformer-based-Models" class="headerlink" title="Transformer based Models"></a>Transformer based Models</h4><h4 id="Deep-Hybrid-Models"><a href="#Deep-Hybrid-Models" class="headerlink" title="Deep Hybrid Models"></a>Deep Hybrid Models</h4><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a><strong>Evaluation</strong></h3><p>ä½œè€…å¦‚ä½•è¯„ä¼°è‡ªå·±çš„æ–¹æ³•ï¼Ÿå®éªŒçš„setupæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿæ„Ÿå…´è¶£å®éªŒæ•°æ®å’Œç»“æœæœ‰å“ªäº›ï¼Ÿæœ‰æ²¡æœ‰é—®é¢˜æˆ–è€…å¯ä»¥å€Ÿé‰´çš„åœ°æ–¹ï¼Ÿ</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h3><p>ä½œè€…ç»™å‡ºäº†å“ªäº›ç»“è®ºï¼Ÿå“ªäº›æ˜¯strong conclusions, å“ªäº›åˆæ˜¯weakçš„conclusionsï¼ˆå³ä½œè€…å¹¶æ²¡æœ‰é€šè¿‡å®éªŒæä¾›evidenceï¼Œåªåœ¨discussionä¸­æåˆ°ï¼›æˆ–å®éªŒçš„æ•°æ®å¹¶æ²¡æœ‰ç»™å‡ºå……åˆ†çš„evidenceï¼‰?</p>
<h3 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a><strong>Notes</strong></h3><p>(optional) ä¸åœ¨ä»¥ä¸Šåˆ—è¡¨ä¸­ï¼Œä½†éœ€è¦ç‰¹åˆ«è®°å½•çš„ç¬”è®°ã€‚</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a><strong>References</strong></h3><p>[1] Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan Srinivasan, and Dragomir R. Radev. 2017. Graph-based Neural Multi-Document Summarization. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017).</p>
<p>[2] Petar VelicÌŒkovicÌ, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph Attention Networks. arXiv preprint arXiv:1710.10903.</p>
<p>[3] Danqing Wang, Pengfei Liu, Yining Zheng, Xipeng Qiu, and Xuanjing Huang. 2020. Heterogeneous Graph Neural Networks for Extractive Document Summarization. arXiv preprint arXiv:2004.12393.</p>
<div class="post-tags"><a class="post-tag-link" href="/myblog/tags/%E8%AE%BA%E6%96%87/" rel="tag">#è®ºæ–‡</a></div></article><div id="paginator"></div></div></div><div id="bottom-outer"><div id="bottom-inner"><hr><div><div><a>2021@ </a><a href="/atom.xml"><img src="/assets/rss.png"></a></div><div id="hexo"><a>Powered by&nbsp</a><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><a>&nbsp&&nbsp</a><a target="_blank" rel="noopener" href="https://github.com/qiantao94/hexo-theme-oasis">Oasis</a></div></div></div></div></div></body><script src="/myblog/js/oasis.js"></script></html>